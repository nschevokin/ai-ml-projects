{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0001196096 Antonelli Giacomo giacomo.antonelli4@studio.unibo.it\n",
    "\n",
    "0001153900 Chevokin Nikita nikita.chevokin@studio.unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNBgGYg_lpVN"
   },
   "source": [
    "# Product Recognition of Books\n",
    "\n",
    "## Image Processing and Computer Vision - Assignment Module \\#1\n",
    "\n",
    "\n",
    "Contacts:\n",
    "\n",
    "- Prof. Giuseppe Lisanti -> giuseppe.lisanti@unibo.it\n",
    "- Prof. Samuele Salti -> samuele.salti@unibo.it\n",
    "- Alex Costanzino -> alex.costanzino@unibo.it\n",
    "- Francesco Ballerini -> francesco.ballerini4@unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R552o2Anyj8T"
   },
   "source": [
    "Computer vision-based object detection techniques can be applied in library or bookstore settings to build a system that identifies books on shelves.\n",
    "\n",
    "Such a system could assist in:\n",
    "* Helping visually impaired users locate books by title/author;\n",
    "* Automating inventory management (e.g., detecting misplaced or out-of-stock books);\n",
    "* Enabling faster book retrieval by recognizing spine text or cover designs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DW42NlZsyTv0"
   },
   "source": [
    "## Task\n",
    "Develop a computer vision system that, given a reference image for each book, is able to identify such book from one picture of a shelf.\n",
    "\n",
    "<figure>\n",
    "<a href=\"https://ibb.co/pvLVjbM5\"><img src=\"https://i.ibb.co/svVx9bNz/example.png\" alt=\"example\" border=\"0\"></a>\n",
    "</figure>\n",
    "\n",
    "For each type of product displayed on the shelf, the system should compute a bounding box aligned with the book spine or cover and report:\n",
    "1. Number of instances;\n",
    "1. Dimension of each instance (area in pixel of the bounding box that encloses each one of them);\n",
    "1. Position in the image reference system of each instance (four corners of the bounding box that enclose them);\n",
    "1. Overlay of the bounding boxes on the scene images.\n",
    "\n",
    "<font color=\"red\"><b>Each step of this assignment must be solved using traditional computer vision techniques.</b></font>\n",
    "\n",
    "#### Example of expected output\n",
    "```\n",
    "Book 0 - 2 instance(s) found:\n",
    "  Instance 1 {top_left: (100,200), top_right: (110, 220), bottom_left: (10, 202), bottom_right: (10, 208), area: 230px}\n",
    "  Instance 2 {top_left: (90,310), top_right: (95, 340), bottom_left: (24, 205), bottom_right: (23, 234), area: 205px}\n",
    "Book 1 â€“ 1 instance(s) found:\n",
    ".\n",
    ".\n",
    ".\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fIbZJKq16ba"
   },
   "source": [
    "## Data\n",
    "Two folders of images are provided:\n",
    "* Models: contains one reference image for each product that the system should be able to identify;\n",
    "* Scenes: contains different shelve pictures to test the developed algorithm in different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "NjP3GCdujYlw",
    "outputId": "e60aa84d-32ea-4bf6-85a8-e0dacd6ca5fa"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!cp -r /content/drive/MyDrive/AssignmentsIPCV/dataset.zip ./\n",
    "!unzip dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Parameters\n",
    "This cell contains all the parameters and feature flags that control the behavior of the book recognition algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If True, perform a search over small rotation angles to refine the bounding box orientation against Canny edges\n",
    "# Improves noticebly the accuracy for some instances, but worse a lot for others because of background edges (so, not enabled for the final version)\n",
    "REFINE_ROTATION = False\n",
    "\n",
    "# Apply CLAHE to model/scene images to reduce the effect of uneven illumination\n",
    "# (This often helps keypoint detection on photographs taken under variable lighting conditions)\n",
    "HANDLE_ILLUMINATION = True\n",
    "\n",
    "\n",
    "# If True, visual intermediate images (matching points, refinement steps) for debugging purpose\n",
    "PLOT_INTERMEDIATE_RESULTS = False\n",
    "\n",
    "# If True, show the model image before showing the final scene results for debugging purpose\n",
    "PLOT_MODEL_BEFORE_SCENE = False\n",
    "\n",
    "\n",
    "# RANSAC reprojection pixel threshold used during findHomography, while the default value is 5, \n",
    "# experimentation found out that some projections were quite inaccurate, has been found that \n",
    "# 1 is a better value, leading to less false positives\n",
    "RANSAC_REPROJ_TRESH = 1\n",
    "\n",
    "# As per the RANSAC value, also the lowe's ratio value has been lowered, granting less false positives\n",
    "LOWES_RATIO = 0.67\n",
    "\n",
    "# Minimum matches required to attempt a detection\n",
    "DETECTION_MIN_MATCH_COUNT = 25\n",
    "ORIG_DETECTION_MIN_MATCH_COUNT = DETECTION_MIN_MATCH_COUNT\n",
    "\n",
    "# After each detection, a mask is applied to the found instance (to try finding other instances in the same image), the \n",
    "# shrink is needed to avoid removing neighboring features related to other instances\n",
    "OCCULTING_SHRINK_RATIO = 0.8\n",
    "\n",
    "# As sometimes, the instances are rotated, a further search for best angle is done, small range \n",
    "# of angles are explored\n",
    "ANGLE_RANGE_DEG = 15\n",
    "ANGLE_STEP = 1\n",
    "\n",
    "# Prevent infinite loops when iteratively masking detections in a scene\n",
    "MAX_DETECTION_ATTEMPTS = 10\n",
    "\n",
    "# IoU threshold for filtering duplicate detections, if IoU between two detections exceeds\n",
    "# this value, the later one is discarded as a duplicate\n",
    "IOU_THRESHOLD = 0.3\n",
    "\n",
    "# Make behavior reproducible\n",
    "cv2.setRNGSeed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# SIFT detector for the instances:\n",
    "# nfeatures=0 (return all detected features up to other constraints)\n",
    "# nOctaveLayers=7 (increased the number of octaves to help with detecting features across a larger range of scales)\n",
    "# contrastThreshold=0.02 (lower threshold yields more keypoints in low-contrast)\n",
    "# edgeThreshold=15 (higher value retains more features along edges, useful for book spines)\n",
    "# sigma=0.8 (halved from default value to preserves fine details, which is effective in combination with a higher nOctaveLayers to detect features on detailed book covers)\n",
    "sift = cv2.SIFT_create(\n",
    "    nfeatures=0,\n",
    "    nOctaveLayers=7,\n",
    "    edgeThreshold=15,\n",
    "    sigma=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "This cell defines all the core utility functions for the book detection pipeline:\n",
    "- kp_detection(): Detects SIFT keypoints and computes descriptors for an image\n",
    "- detect_object(): Performs feature matching and homography estimation between model and scene\n",
    "- refine_bbox_rotation(): Optimizes bounding box orientation using Canny edge alignment\n",
    "- compute_iou(): Calculates intersection-over-union to avoid duplicate detections\n",
    "- compute_quad_area(): Calculates the area of a quadrilateral bounding box\n",
    "- get_labeled_corners(): Labels corners as top-left, top-right, bottom-left, bottom-right\n",
    "- plot_rgb_image(): Helper for displaying images with matplotlib\n",
    "- draw_matching_points(): Visualizes matched keypoints for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "46OHXwSYeY2w",
    "outputId": "899af484-10f5-425e-c8f6-dc5e0cf70a9f"
   },
   "outputs": [],
   "source": [
    "def kp_detection(img):\n",
    "    \"\"\"Detect SIFT keypoints and compute descriptors for the given image.\"\"\"\n",
    "    kp = sift.detect(img)\n",
    "    \n",
    "    img_kp, img_des = sift.compute(img, kp)\n",
    "    return img_kp, img_des\n",
    "\n",
    "def detect_object(model_kp, model_des, scene_kp, scene_des, min_match_count=4):\n",
    "\t\"\"\" Given keypoints and descriptors for a model and a scene, find matching\n",
    "\tkeypoints and estimate a homography matrix if enough matches are found.\n",
    "\t\"\"\"\n",
    "\t# Finding the two best matches for each descriptor\n",
    "\tFLANN_INDEX_KDTREE = 1\n",
    "\tindex_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "\tsearch_params = dict(checks=50, random_seed=42)  # Add random_seed for reproducibility\n",
    "\tflann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\tmatches = flann.knnMatch(model_des, scene_des, k=2)\n",
    "\n",
    "\t# Apply Lowe's ratio test on the matches\n",
    "\tgood_matches = [m for m, n in matches if m.distance < LOWES_RATIO * n.distance]\n",
    "\n",
    "\tif len(good_matches) < min_match_count:\n",
    "\t\t# Not enough matches to continue\n",
    "\t\treturn []\n",
    "\n",
    "\tdetections = []\n",
    "\n",
    "\t# Prepare point arrays for homography estimation (model->scene)\n",
    "\tsrc_pts = np.float32([model_kp[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\tdst_pts = np.float32([scene_kp[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "\t# Estimate homography with RANSAC to discard outliers among the matched keypoints\n",
    "\tM, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, RANSAC_REPROJ_TRESH)\n",
    "\n",
    "\tif M is not None:\n",
    "\t\t# Record number of matches used, homography, and the matches themselves\n",
    "\t\tdetections.append((len(good_matches), M, good_matches))\n",
    "\n",
    "\treturn detections\n",
    "\n",
    "\n",
    "def refine_bbox_rotation(initial_corners, canny_edges, angle_range=15, angle_step=1):\n",
    "    \"\"\"This function performs a search over different degrees of rotation angles to select\n",
    "    the angle whose polygon overlaps the most with the Canny edges of the scene.\n",
    "\n",
    "    note: This assumes object boundaries correspond to strong Canny edges which is not always the case,\n",
    "    indeed even if there are improvements in most of the instances, there are few cases where the result\n",
    "    is worse than the original (mainly due to the fact that some rotations have overlapping edge points\n",
    "    with edges not related to the instance, i.e. some other books, which leads to confusion).\n",
    "    \"\"\"\n",
    "    best_score = -np.inf\n",
    "    best_corners = initial_corners\n",
    "\n",
    "    # Centroid of the bounding box (used as the rotation center)\n",
    "    centroid = tuple(np.mean(initial_corners, axis=0))\n",
    "\n",
    "    # Larger angle penalty factor\n",
    "    perimeter_approx = 0\n",
    "    for i in range(4):\n",
    "        perimeter_approx += np.linalg.norm(initial_corners[i] - initial_corners[(i + 1) % 4])\n",
    "    penalty_factor = perimeter_approx / 25.0\n",
    "\n",
    "    # Trying a small range of rotations + computation of a simple edge-alignment score\n",
    "    for angle in range(-angle_range, angle_range + 1, angle_step):\n",
    "        rot_mat = cv2.getRotationMatrix2D(centroid, angle, 1.0)\n",
    "        reshaped_corners = initial_corners.reshape(-1, 1, 2)\n",
    "        rotated_corners = cv2.transform(reshaped_corners, rot_mat).reshape(4, 2)\n",
    "\n",
    "        # Create a mask with the rotated polygon and count how many edge pixels\n",
    "        # coincide with the polygon boundary. Thicker polylines help tolerate\n",
    "        # small misalignments.\n",
    "        mask = np.zeros_like(canny_edges)\n",
    "        cv2.polylines(mask, [np.int32(rotated_corners)], isClosed=True, color=255, thickness=3)\n",
    "        intersection = cv2.bitwise_and(mask, canny_edges)\n",
    "        raw_edge_score = np.sum(intersection > 0)\n",
    "\n",
    "        # Final score combines edge overlap and an angle penalty to prevent\n",
    "        # arbitrary large rotations.\n",
    "        score = raw_edge_score - (abs(angle) * penalty_factor)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_corners = rotated_corners\n",
    "\n",
    "    return best_corners\n",
    "\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    \"\"\"Compute intersection-over-union between two bounding boxes, by rasterizing them,\n",
    "    then computing the IoU pixel-wise.\n",
    "    \"\"\"\n",
    "    h, w = 1000, 1000\n",
    "    mask1 = np.zeros((h, w), dtype=np.uint8)\n",
    "    mask2 = np.zeros((h, w), dtype=np.uint8)\n",
    "    cv2.fillPoly(mask1, [np.int32(box1)], color=255)\n",
    "    cv2.fillPoly(mask2, [np.int32(box2)], color=255)\n",
    "    intersection = cv2.bitwise_and(mask1, mask2)\n",
    "    union = cv2.bitwise_or(mask1, mask2)\n",
    "    inter_area = np.sum(intersection > 0)\n",
    "    union_area = np.sum(union > 0)\n",
    "    if union_area == 0:\n",
    "        return 0.0\n",
    "    return inter_area / union_area\n",
    "\n",
    "\n",
    "def compute_quad_area(corners):\n",
    "\t\"\"\"Compute the area of a quadrilateral defined by its corner points.\"\"\"\n",
    "\tn = len(corners)\n",
    "\tarea = 0.0\n",
    "\tfor i in range(n):\n",
    "\t\tj = (i + 1) % n\n",
    "\t\tarea += corners[i][0] * corners[j][1]\n",
    "\t\tarea -= corners[j][0] * corners[i][1]\n",
    "\tarea = abs(area) / 2.0\n",
    "\treturn area\n",
    "\n",
    "\n",
    "def get_labeled_corners(corners):\n",
    "    \"\"\"\n",
    "    Label the corners of a quadrilateral defined by its corner points.\n",
    "    \"\"\"\n",
    "    points = corners.tolist()\n",
    "    points.sort(key=lambda p: p[1])\n",
    "    top_points = points[:2]\n",
    "    bottom_points = points[2:]\n",
    "    top_points.sort(key=lambda p: p[0])\n",
    "    bottom_points.sort(key=lambda p: p[0])\n",
    "    tl = (int(round(top_points[0][0])), int(round(top_points[0][1])))\n",
    "    tr = (int(round(top_points[1][0])), int(round(top_points[1][1])))\n",
    "    bl = (int(round(bottom_points[0][0])), int(round(bottom_points[0][1])))\n",
    "    br = (int(round(bottom_points[1][0])), int(round(bottom_points[1][1])))\n",
    "    return {\n",
    "        'top_left': tl,\n",
    "        'top_right': tr,\n",
    "        'bottom_left': bl,\n",
    "        'bottom_right': br\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_rgb_image(img_rgb, title=None):\n",
    "    \"\"\"Helper to plot RGB images\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(img_rgb)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def draw_matching_points(scene_img_gray, scene_kp, good_matches):\n",
    "    \"\"\"Draw green circles at each matched scene keypoint for visualization\"\"\"\n",
    "    scene_img = cv2.cvtColor(scene_img_gray, cv2.COLOR_GRAY2BGR)\n",
    "    for match in good_matches:\n",
    "        scene_pt = scene_kp[match.trainIdx].pt\n",
    "        center = (int(scene_pt[0]), int(scene_pt[1]))\n",
    "        cv2.circle(scene_img, center, radius=3, color=(0, 255, 0), thickness=2)\n",
    "    return scene_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing & Feature Extraction\n",
    "This cell loads all model and scene images and preprocesses them for the detection pipeline:\n",
    "\n",
    "Process:\n",
    "1. Load Images: Reads all 22 model images and 29 scene images from the dataset\n",
    "2. Apply CLAHE: Optionally applies Contrast Limited Adaptive Histogram Equalization to handle varying illumination conditions\n",
    "3. Extract Features: Computes SIFT keypoints and descriptors for all images (done once for efficiency)\n",
    "4. Generate Canny Edges: Creates edge maps for scenes (used in rotation refinement)\n",
    "\n",
    "This preprocessing step optimizes the pipeline by computing features once rather than repeatedly during detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing model descriptors\n",
      "Computing scene descriptors\n",
      "Computing scene descriptors\n"
     ]
    }
   ],
   "source": [
    "# Load images and scenes\n",
    "img_models = [f\"dataset/models/model_{n}.png\" for n in range(22)]\n",
    "img_scenes = [f\"dataset/scenes/scene_{n}.jpg\" for n in range(29)]\n",
    "\n",
    "# Extract keypoints and descriptors from model images (optimization to do it once for all)\n",
    "models_data = []\n",
    "scenes_data = []\n",
    "\n",
    "print(\"Computing model descriptors\")\n",
    "for model_path in img_models:\n",
    "    img_model_gray = cv2.imread(model_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img_model_gray is None:\n",
    "        print(f\"Problem loading model image {model_path}\")\n",
    "        continue\n",
    "\n",
    "    # Optionally apply CLAHE to reduce illumination differences between model\n",
    "    # photos and scene photos. This helps SIFT find more stable keypoints under\n",
    "    # variable lighting.\n",
    "    if HANDLE_ILLUMINATION:\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        img_model_gray = clahe.apply(img_model_gray)\n",
    "\n",
    "    model_kp, model_des = kp_detection(img_model_gray)\n",
    "    models_data.append((model_path, model_kp, model_des, img_model_gray))\n",
    "\n",
    "# Extract keypoints, descriptors and Canny's edges from scene images (optimization to do it once for all)\n",
    "print(\"Computing scene descriptors\")\n",
    "for scene_path in img_scenes:\n",
    "    img_scene_gray = cv2.imread(scene_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img_scene_gray is None:\n",
    "        print(f\"Problem loading scene image {scene_path}\")\n",
    "        continue\n",
    "\n",
    "    if HANDLE_ILLUMINATION:\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        img_scene_gray = clahe.apply(img_scene_gray)\n",
    "\n",
    "    scene_kp, scene_des = kp_detection(img_scene_gray)\n",
    "    \n",
    "    canny_scene = cv2.Canny(img_scene_gray, 50, 150)\n",
    "    canny_scene = cv2.dilate(canny_scene, np.ones((3, 3), np.uint8))\n",
    "    scenes_data.append((scene_path, scene_kp, scene_des, img_scene_gray, canny_scene))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Detection Pipeline\n",
    "This is the core detection algorithm that identifies book instances in shelf scenes:\n",
    "1. Model-Scene Matching: For each model, match it against all scenes using SIFT features\n",
    "2. Iterative Detection: Use progressive masking to find multiple instances of the same book\n",
    "3. Homography Estimation: Transform model coordinates to scene coordinates using RANSAC\n",
    "4. Bounding Box Refinement: Optionally refine orientation using Canny edge alignment\n",
    "5. Duplicate Filtering: Use IoU thresholding to avoid detecting the same instance multiple times\n",
    "6. Masking: Mask detected regions to enable finding additional instances\n",
    "7. Results Collection: Store detection coordinates, areas, and generate visualization images\n",
    "\n",
    "The algorithm generates detailed reports for each book with instance counts, coordinates, and areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing model_0.png:\n",
      "\tscene_26.jpg - Detection 1: 400 good matches\n",
      "\tscene_26.jpg - Detection 2: 197 good matches\n",
      "Book 0 - 2 instance(s) found:\n",
      "Instance 1 in scene_26 {top_left: (210, 181), top_right: (244, 181), bottom_left: (210, 568), bottom_right: (244, 568), area: 13235px}\n",
      "Instance 2 in scene_26 {top_left: (170, 181), top_right: (205, 181), bottom_left: (176, 569), bottom_right: (210, 571), area: 13486px}\n",
      "\n",
      "Processing model_1.png:\n",
      "\tscene_28.jpg - Detection 1: 174 good matches\n",
      "\tscene_28.jpg - Detection 2: 78 good matches\n",
      "Book 1 - 2 instance(s) found:\n",
      "Instance 1 in scene_28 {top_left: (207, 248), top_right: (236, 248), bottom_left: (207, 554), bottom_right: (236, 555), area: 8846px}\n",
      "Instance 2 in scene_28 {top_left: (237, 248), top_right: (266, 244), bottom_left: (237, 551), bottom_right: (266, 552), area: 8878px}\n",
      "\n",
      "Processing model_2.png:\n",
      "\tscene_27.jpg - Detection 1: 443 good matches\n",
      "Book 2 - 1 instance(s) found:\n",
      "Instance 1 in scene_27 {top_left: (258, 146), top_right: (303, 146), bottom_left: (258, 564), bottom_right: (303, 564), area: 18847px}\n",
      "\n",
      "Processing model_3.png:\n",
      "\tscene_27.jpg - Detection 1: 187 good matches\n",
      "\tscene_27.jpg - Detection 2: 62 good matches\n",
      "Book 3 - 2 instance(s) found:\n",
      "Instance 1 in scene_27 {top_left: (416, 144), top_right: (466, 143), bottom_left: (401, 567), bottom_right: (449, 568), area: 20891px}\n",
      "Instance 2 in scene_27 {top_left: (356, 21), top_right: (431, 17), bottom_left: (354, 575), bottom_right: (402, 575), area: 34239px}\n",
      "\n",
      "Processing model_4.png:\n",
      "\tscene_8.jpg - Detection 1: 26 good matches\n",
      "\tscene_26.jpg - Detection 1: 1705 good matches\n",
      "Book 4 - 2 instance(s) found:\n",
      "Instance 1 in scene_8 {top_left: (167, 232), top_right: (167, 232), bottom_left: (167, 232), bottom_right: (168, 234), area: 0px}\n",
      "Instance 2 in scene_26 {top_left: (249, 183), top_right: (433, 183), bottom_left: (248, 527), bottom_right: (433, 527), area: 63307px}\n",
      "\n",
      "Processing model_5.png:\n",
      "\tscene_23.jpg - Detection 1: 1415 good matches\n",
      "Book 5 - 1 instance(s) found:\n",
      "Instance 1 in scene_23 {top_left: (342, 149), top_right: (392, 149), bottom_left: (342, 572), bottom_right: (392, 572), area: 20861px}\n",
      "\n",
      "Processing model_6.png:\n",
      "\tscene_19.jpg - Detection 1: 357 good matches\n",
      "\tscene_19.jpg - Detection 2: 127 good matches\n",
      "\tscene_19.jpg - Detection 3: 107 good matches\n",
      "Book 6 - 3 instance(s) found:\n",
      "Instance 1 in scene_19 {top_left: (385, 32), top_right: (411, 32), bottom_left: (385, 580), bottom_right: (411, 580), area: 14418px}\n",
      "Instance 2 in scene_19 {top_left: (439, 22), top_right: (465, 24), bottom_left: (439, 572), bottom_right: (464, 573), area: 14175px}\n",
      "Instance 3 in scene_19 {top_left: (412, 31), top_right: (438, 33), bottom_left: (409, 584), bottom_right: (436, 583), area: 14491px}\n",
      "\n",
      "Processing model_7.png:\n",
      "\tscene_19.jpg - Detection 1: 253 good matches\n",
      "\tscene_19.jpg - Detection 2: 119 good matches\n",
      "Book 7 - 2 instance(s) found:\n",
      "Instance 1 in scene_19 {top_left: (497, 40), top_right: (534, 40), bottom_left: (497, 588), bottom_right: (534, 588), area: 20087px}\n",
      "Instance 2 in scene_19 {top_left: (458, 46), top_right: (495, 42), bottom_left: (460, 581), bottom_right: (496, 586), area: 19791px}\n",
      "\n",
      "Processing model_8.png:\n",
      "\tscene_18.jpg - Detection 1: 307 good matches\n",
      "\tscene_18.jpg - Detection 2: 164 good matches\n",
      "\tscene_18.jpg - Detection 3: 145 good matches\n",
      "Book 8 - 3 instance(s) found:\n",
      "Instance 1 in scene_18 {top_left: (291, 131), top_right: (319, 131), bottom_left: (291, 587), bottom_right: (319, 587), area: 12531px}\n",
      "Instance 2 in scene_18 {top_left: (315, 134), top_right: (342, 133), bottom_left: (317, 587), bottom_right: (345, 589), area: 12416px}\n",
      "Instance 3 in scene_18 {top_left: (266, 128), top_right: (294, 128), bottom_left: (265, 586), bottom_right: (293, 585), area: 12841px}\n",
      "\n",
      "Processing model_9.png:\n",
      "\tscene_18.jpg - Detection 1: 312 good matches\n",
      "\tscene_18.jpg - Detection 2: 168 good matches\n",
      "\tscene_18.jpg - Detection 3: 150 good matches\n",
      "Book 9 - 3 instance(s) found:\n",
      "Instance 1 in scene_18 {top_left: (316, 131), top_right: (343, 131), bottom_left: (316, 587), bottom_right: (343, 587), area: 12445px}\n",
      "Instance 2 in scene_18 {top_left: (267, 126), top_right: (295, 126), bottom_left: (262, 584), bottom_right: (290, 585), area: 12764px}\n",
      "Instance 3 in scene_18 {top_left: (292, 130), top_right: (319, 127), bottom_left: (290, 584), bottom_right: (317, 589), area: 12463px}\n",
      "\n",
      "Processing model_10.png:\n",
      "\tscene_18.jpg - Detection 1: 241 good matches\n",
      "\tscene_18.jpg - Detection 2: 152 good matches\n",
      "\tscene_18.jpg - Detection 3: 131 good matches\n",
      "Book 10 - 3 instance(s) found:\n",
      "Instance 1 in scene_18 {top_left: (109, 39), top_right: (136, 39), bottom_left: (109, 585), bottom_right: (136, 585), area: 15069px}\n",
      "Instance 2 in scene_18 {top_left: (139, 43), top_right: (166, 41), bottom_left: (139, 584), bottom_right: (167, 588), area: 14784px}\n",
      "Instance 3 in scene_18 {top_left: (78, 37), top_right: (105, 38), bottom_left: (79, 587), bottom_right: (106, 584), area: 14743px}\n",
      "\n",
      "Processing model_11.png:\n",
      "\tscene_5.jpg - Detection 1: 25 good matches\n",
      "\tscene_15.jpg - Detection 1: 151 good matches\n",
      "\tscene_15.jpg - Detection 2: 55 good matches\n",
      "\tscene_16.jpg - Detection 1: 155 good matches\n",
      "\tscene_16.jpg - Detection 2: 52 good matches\n",
      "\tscene_17.jpg - Detection 1: 503 good matches\n",
      "\tscene_17.jpg - Detection 2: 90 good matches\n",
      "Book 11 - 7 instance(s) found:\n",
      "Instance 1 in scene_5 {top_left: (88, 262), top_right: (107, 283), bottom_left: (124, 316), bottom_right: (142, 334), area: 10px}\n",
      "Instance 2 in scene_15 {top_left: (258, 189), top_right: (300, 185), bottom_left: (252, 530), bottom_right: (293, 539), area: 14374px}\n",
      "Instance 3 in scene_15 {top_left: (217, 181), top_right: (260, 188), bottom_left: (211, 554), bottom_right: (254, 531), area: 15404px}\n",
      "Instance 4 in scene_16 {top_left: (257, 187), top_right: (299, 188), bottom_left: (251, 535), bottom_right: (293, 536), area: 14542px}\n",
      "Instance 5 in scene_16 {top_left: (218, 182), top_right: (260, 188), bottom_left: (210, 550), bottom_right: (254, 534), area: 15371px}\n",
      "Instance 6 in scene_17 {top_left: (255, 188), top_right: (296, 188), bottom_left: (255, 536), bottom_right: (296, 536), area: 14441px}\n",
      "Instance 7 in scene_17 {top_left: (216, 187), top_right: (258, 185), bottom_left: (216, 531), bottom_right: (259, 538), area: 14765px}\n",
      "\n",
      "Processing model_12.png:\n",
      "\tscene_15.jpg - Detection 1: 78 good matches\n",
      "\tscene_15.jpg - Detection 2: 51 good matches\n",
      "\tscene_15.jpg - Detection 3: 34 good matches\n",
      "\tscene_16.jpg - Detection 1: 71 good matches\n",
      "\tscene_16.jpg - Detection 2: 51 good matches\n",
      "\tscene_16.jpg - Detection 3: 34 good matches\n",
      "\tscene_17.jpg - Detection 1: 216 good matches\n",
      "\tscene_17.jpg - Detection 2: 94 good matches\n",
      "\tscene_17.jpg - Detection 3: 58 good matches\n",
      "Book 12 - 9 instance(s) found:\n",
      "Instance 1 in scene_15 {top_left: (500, 215), top_right: (530, 215), bottom_left: (495, 527), bottom_right: (524, 529), area: 9272px}\n",
      "Instance 2 in scene_15 {top_left: (526, 218), top_right: (555, 213), bottom_left: (518, 528), bottom_right: (548, 540), area: 9603px}\n",
      "Instance 3 in scene_15 {top_left: (474, 214), top_right: (505, 213), bottom_left: (468, 524), bottom_right: (500, 530), area: 9843px}\n",
      "Instance 4 in scene_16 {top_left: (500, 214), top_right: (530, 216), bottom_left: (495, 528), bottom_right: (524, 528), area: 9241px}\n",
      "Instance 5 in scene_16 {top_left: (525, 216), top_right: (555, 214), bottom_left: (518, 524), bottom_right: (548, 531), area: 9331px}\n",
      "Instance 6 in scene_16 {top_left: (472, 213), top_right: (507, 214), bottom_left: (466, 527), bottom_right: (501, 532), area: 11050px}\n",
      "Instance 7 in scene_17 {top_left: (498, 211), top_right: (527, 211), bottom_left: (498, 524), bottom_right: (527, 524), area: 9192px}\n",
      "Instance 8 in scene_17 {top_left: (523, 213), top_right: (552, 208), bottom_left: (523, 510), bottom_right: (551, 522), area: 8730px}\n",
      "Instance 9 in scene_17 {top_left: (471, 211), top_right: (502, 209), bottom_left: (471, 530), bottom_right: (503, 531), area: 10060px}\n",
      "\n",
      "Processing model_13.png:\n",
      "\tscene_5.jpg - Detection 1: 743 good matches\n",
      "Book 13 - 1 instance(s) found:\n",
      "Instance 1 in scene_5 {top_left: (103, 70), top_right: (146, 70), bottom_left: (103, 573), bottom_right: (146, 573), area: 21935px}\n",
      "\n",
      "Processing model_14.png:\n",
      "\tscene_4.jpg - Detection 1: 826 good matches\n",
      "\tscene_4.jpg - Detection 2: 270 good matches\n",
      "Book 14 - 2 instance(s) found:\n",
      "Instance 1 in scene_4 {top_left: (92, 2), top_right: (136, 2), bottom_left: (92, 618), bottom_right: (136, 618), area: 26755px}\n",
      "Instance 2 in scene_4 {top_left: (56, -2), top_right: (99, -4), bottom_left: (60, 616), bottom_right: (103, 623), area: 26790px}\n",
      "\n",
      "Processing model_15.png:\n",
      "\tscene_4.jpg - Detection 1: 666 good matches\n",
      "\tscene_4.jpg - Detection 2: 313 good matches\n",
      "Book 15 - 2 instance(s) found:\n",
      "Instance 1 in scene_4 {top_left: (227, 49), top_right: (268, 49), bottom_left: (227, 609), bottom_right: (268, 609), area: 23267px}\n",
      "Instance 2 in scene_4 {top_left: (192, 44), top_right: (234, 46), bottom_left: (197, 615), bottom_right: (240, 611), area: 24218px}\n",
      "\n",
      "Processing model_16.png:\n",
      "\tscene_3.jpg - Detection 1: 506 good matches\n",
      "\tscene_3.jpg - Detection 2: 122 good matches\n",
      "Book 16 - 2 instance(s) found:\n",
      "Instance 1 in scene_3 {top_left: (377, 210), top_right: (426, 210), bottom_left: (377, 543), bottom_right: (426, 543), area: 16244px}\n",
      "Instance 2 in scene_3 {top_left: (425, 210), top_right: (475, 210), bottom_left: (419, 552), bottom_right: (469, 547), area: 16873px}\n",
      "\n",
      "Processing model_17.png:\n",
      "\tscene_2.jpg - Detection 1: 375 good matches\n",
      "Book 17 - 1 instance(s) found:\n",
      "Instance 1 in scene_2 {top_left: (284, 25), top_right: (319, 25), bottom_left: (284, 504), bottom_right: (318, 503), area: 16618px}\n",
      "\n",
      "Processing model_18.png:\n",
      "\tscene_1.jpg - Detection 1: 177 good matches\n",
      "\tscene_1.jpg - Detection 2: 105 good matches\n",
      "Book 18 - 2 instance(s) found:\n",
      "Instance 1 in scene_1 {top_left: (442, 43), top_right: (491, 42), bottom_left: (442, 521), bottom_right: (491, 521), area: 23606px}\n",
      "Instance 2 in scene_1 {top_left: (489, 40), top_right: (538, 40), bottom_left: (482, 512), bottom_right: (531, 513), area: 22965px}\n",
      "\n",
      "Processing model_19.png:\n",
      "\tscene_9.jpg - Detection 1: 68 good matches\n",
      "\tscene_9.jpg - Detection 2: 55 good matches\n",
      "\tscene_9.jpg - Detection 3: 50 good matches\n",
      "\tscene_9.jpg - Detection 4: 57 good matches\n",
      "\tscene_10.jpg - Detection 1: 210 good matches\n",
      "\tscene_10.jpg - Detection 2: 74 good matches\n",
      "\tscene_10.jpg - Detection 3: 69 good matches\n",
      "\tscene_10.jpg - Detection 4: 64 good matches\n",
      "Book 19 - 8 instance(s) found:\n",
      "Instance 1 in scene_9 {top_left: (138, 339), top_right: (435, 329), bottom_left: (137, 367), bottom_right: (444, 357), area: 8580px}\n",
      "Instance 2 in scene_9 {top_left: (141, 313), top_right: (437, 306), bottom_left: (127, 340), bottom_right: (440, 332), area: 8055px}\n",
      "Instance 3 in scene_9 {top_left: (143, 367), top_right: (438, 356), bottom_left: (134, 395), bottom_right: (442, 385), area: 8531px}\n",
      "Instance 4 in scene_9 {top_left: (132, 393), top_right: (447, 381), bottom_left: (149, 420), bottom_right: (439, 410), area: 8447px}\n",
      "Instance 5 in scene_10 {top_left: (332, 203), top_right: (360, 203), bottom_left: (332, 504), bottom_right: (360, 504), area: 8407px}\n",
      "Instance 6 in scene_10 {top_left: (308, 204), top_right: (335, 203), bottom_left: (306, 504), bottom_right: (334, 510), area: 8305px}\n",
      "Instance 7 in scene_10 {top_left: (362, 200), top_right: (387, 204), bottom_left: (362, 504), bottom_right: (386, 505), area: 7346px}\n",
      "Instance 8 in scene_10 {top_left: (386, 201), top_right: (414, 201), bottom_left: (386, 503), bottom_right: (414, 502), area: 8351px}\n",
      "\n",
      "Processing model_20.png:\n",
      "\tscene_7.jpg - Detection 1: 467 good matches\n",
      "\tscene_7.jpg - Detection 2: 218 good matches\n",
      "Book 20 - 2 instance(s) found:\n",
      "Instance 1 in scene_7 {top_left: (236, 173), top_right: (290, 173), bottom_left: (236, 556), bottom_right: (290, 556), area: 20643px}\n",
      "Instance 2 in scene_7 {top_left: (279, 170), top_right: (336, 168), bottom_left: (276, 559), bottom_right: (331, 563), area: 21804px}\n",
      "\n",
      "Processing model_21.png:\n",
      "\tscene_6.jpg - Detection 1: 651 good matches\n",
      "Book 21 - 1 instance(s) found:\n",
      "Instance 1 in scene_6 {top_left: (217, 74), top_right: (271, 74), bottom_left: (217, 564), bottom_right: (271, 564), area: 26358px}\n"
     ]
    }
   ],
   "source": [
    "book_detections = defaultdict(list)\n",
    "\n",
    "for model_path, model_kp, model_des, model_img in models_data:\n",
    "    print(f\"\\nProcessing {model_path.split('/')[-1]}:\")\n",
    "    for scene_path, scene_kp, scene_des, scene_img_gray_mutable, canny_scene in scenes_data:\n",
    "        # As some of the scene is going to be masked, to be able to find multiple instances, we \n",
    "        # work on copies of keypoints/descriptors\n",
    "        current_scene_kp = scene_kp\n",
    "        current_scene_des = scene_des\n",
    "\n",
    "        original_scene_img = cv2.imread(scene_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if HANDLE_ILLUMINATION:\n",
    "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "            original_scene_img = clahe.apply(original_scene_img)\n",
    "        scene_img_gray_for_masking = scene_img_gray_mutable.copy()\n",
    "\n",
    "        detection_count = 0\n",
    "        all_detections = []\n",
    "        current_min_matches = DETECTION_MIN_MATCH_COUNT\n",
    "        attempt_count = 0\n",
    "\n",
    "        # Iteratively detect and mask found instances until no more are found or\n",
    "        # we reach a safety attempt limit to avoid possible infinite loops\n",
    "        while True:\n",
    "            attempt_count += 1\n",
    "            if attempt_count > MAX_DETECTION_ATTEMPTS:\n",
    "                #print(f\"\\t\\tMax detection attempts reached for {scene_path.split('/')[-1]}. Stopping.\")\n",
    "                break\n",
    "\n",
    "            detections = detect_object(\n",
    "                model_kp, model_des, current_scene_kp, current_scene_des,\n",
    "                min_match_count=current_min_matches\n",
    "            )\n",
    "\n",
    "            # If no more detections in the current (masked) scene\n",
    "            if not detections:\n",
    "                break\n",
    "\n",
    "            # Keep polygons to mask after processing all clusters (so clusters do\n",
    "            # not interfere with each other within the same iteration)\n",
    "            to_mask = []\n",
    "            \n",
    "            for det in detections:\n",
    "                n_matches, M, good_matches = det\n",
    "\n",
    "                # Optional visualization of matched scene keypoints\n",
    "                if PLOT_INTERMEDIATE_RESULTS:\n",
    "                    scene_with_points = draw_matching_points(original_scene_img, current_scene_kp, good_matches)\n",
    "                    plot_rgb_image(cv2.cvtColor(scene_with_points, cv2.COLOR_BGR2RGB), \n",
    "\t\t\t\t\t\t\t\tf\"Matching Points in {scene_path.split('/')[-1]}\")\n",
    "\n",
    "                # Build the model's bounding rectangle and project it into the scene\n",
    "                h, w = model_img.shape[:2]\n",
    "                model_corners = np.float32([[0, 0], [w, 0], [w, h], [0, h]]).reshape(-1, 1, 2)\n",
    "                initial_scene_corners = cv2.perspectiveTransform(model_corners, M).reshape(4, 2)\n",
    "\n",
    "                # Optionally box refinement using image edges\n",
    "                if REFINE_ROTATION:\n",
    "                    print(\"\\t\\tRefining bounding box rotation...\")\n",
    "                    final_scene_corners = refine_bbox_rotation(\n",
    "                        initial_scene_corners, canny_scene,\n",
    "                        ANGLE_RANGE_DEG, ANGLE_STEP\n",
    "                    )\n",
    "                else:\n",
    "                    final_scene_corners = initial_scene_corners\n",
    "\n",
    "                # Check overlap with previously accepted detections using IoU function, if \n",
    "                # the IoU is high enough we skip this detection to avoid duplicates\n",
    "                overlap = False\n",
    "                for prev_box in all_detections:\n",
    "                    if compute_iou(final_scene_corners, prev_box) > IOU_THRESHOLD:\n",
    "                        if PLOT_INTERMEDIATE_RESULTS:\n",
    "                            print(f\"\\t\\tSkipping detection due to high overlap with previous.\")\n",
    "                        \n",
    "                        overlap = True\n",
    "                        break\n",
    "\n",
    "                if not overlap:\n",
    "                    detection_count += 1\n",
    "                    print(f\"\\t{scene_path.split('/')[-1]} - Detection {detection_count}: {n_matches} good matches\")\n",
    "                    all_detections.append(final_scene_corners)\n",
    "                    to_mask.append(final_scene_corners)\n",
    "\n",
    "                # Visualization comparison between initial and refined box\n",
    "                if PLOT_INTERMEDIATE_RESULTS and REFINE_ROTATION:\n",
    "                        vis_img = cv2.cvtColor(original_scene_img, cv2.COLOR_GRAY2BGR)\n",
    "                        cv2.polylines(vis_img, [np.int32(initial_scene_corners)], isClosed=True, color=(0, 0, 255), thickness=2)\n",
    "                        cv2.polylines(vis_img, [np.int32(final_scene_corners)], isClosed=True, color=(0, 255, 0), thickness=3)\n",
    "                        plot_rgb_image(cv2.cvtColor(vis_img, cv2.COLOR_BGR2RGB), f\"Rotation Refinement (Red: Initial, Green: Refined) in {scene_path.split('/')[-1]}\")\n",
    "\n",
    "            # After the processing of the instance, mask it from the scene image such that\n",
    "            # further iterations will not rediscover the same instances\n",
    "            # note: as part of the found image might be useful for another very close detection,\n",
    "            # a smaller part of the image is occulted instead (defined by OCCULTING_SHRINK_RATIO variable)\n",
    "            for corners in to_mask:\n",
    "                centroid = np.mean(corners, axis=0)\n",
    "                mask_corners = centroid + OCCULTING_SHRINK_RATIO * (corners - centroid)\n",
    "                mask_polygon = np.int32([mask_corners])\n",
    "                cv2.fillPoly(scene_img_gray_for_masking, mask_polygon, color=0)\n",
    "\n",
    "            # Recompute keypoints/descriptors on the masked image for the next iteration\n",
    "            current_scene_kp, current_scene_des = kp_detection(scene_img_gray_for_masking)\n",
    "\n",
    "        # If we found at least one detection, save/plot combined results\n",
    "        if detection_count > 0:\n",
    "            if PLOT_MODEL_BEFORE_SCENE:\n",
    "                plot_rgb_image(cv2.cvtColor(model_img, cv2.COLOR_BGR2RGB),\n",
    "                               f\"Model Image: {model_path.split('/')[-1]}\")\n",
    "                \n",
    "                plot_rgb_image(cv2.cvtColor(final_img, cv2.COLOR_BGR2RGB),\n",
    "\t\t\t\t\t\t\tf\"All Detections in {scene_path.split('/')[-1]}\")\n",
    "    \n",
    "            final_img = cv2.cvtColor(original_scene_img, cv2.COLOR_GRAY2BGR)\n",
    "            for box in all_detections:\n",
    "                cv2.polylines(final_img, [np.int32(box)], isClosed=True, color=(0, 255, 0), thickness=3)\n",
    "\n",
    "            # Save final visualization for bounding box visualization \n",
    "            # Filenames contain the model and scene identifiers plus how many instances were detected.\n",
    "            os.makedirs(\"final_detections\", exist_ok=True)\n",
    "            model_name = (model_path.split('/')[-1]).split('.')[0]\n",
    "            scene_name = (scene_path.split('/')[-1]).split('.')[0]\n",
    "            cv2.imwrite(f\"final_detections/{model_name}_{scene_name}_x{detection_count}.png\", final_img)\n",
    "\n",
    "            book_id = model_name.split('_')[1]\n",
    "            for box in all_detections:\n",
    "                labeled = get_labeled_corners(box)\n",
    "                area = compute_quad_area(box)\n",
    "                book_detections[book_id].append((scene_name, labeled, area))\n",
    "\n",
    "    # After processing all scenes for this model, print the aggregated report\n",
    "    book_id = model_name.split('_')[1]\n",
    "    if book_detections[book_id]:\n",
    "        total = len(book_detections[book_id])\n",
    "        print(f\"Book {book_id} - {total} instance(s) found:\")\n",
    "        for i, (scene, labeled, area) in enumerate(book_detections[book_id], 1):\n",
    "            tl = labeled['top_left']\n",
    "            tr = labeled['top_right']\n",
    "            bl = labeled['bottom_left']\n",
    "            br = labeled['bottom_right']\n",
    "            print(f\"Instance {i} in {scene} {{top_left: ({tl[0]}, {tl[1]}), top_right: ({tr[0]}, {tr[1]}), bottom_left: ({bl[0]}, {bl[1]}), bottom_right: ({br[0]}, {br[1]}), area: {int(area)}px}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous Attempts\n",
    "As the task ask for the project to be coincisive all of the tries with different techniques have been deleted, but here is a list of what has been tried and the encountered problems:\n",
    "- Template Matching: different tecniques of template matching has been tried, among which ZNCC gave the best results, especially in scene/models with intensity changes, however it struggled with different scale models like scene_27, and other cases\n",
    "- DBSCAN: During the exploration to improve SIFT method, DBSCAN has been used after the keypoints matching phase, as sometimes there were keypoints really off, from the actual model, using DBSCAN, in particular its property of eliminating outliers helped generating better homographies, however the cluster of points were different for some pairs of model/scene, which led to be impossible to find the best parameter, then, by tuning more accuratelty parameters of the SIFT model the outliers points problem has been solved anyway, without the necessity of DBSCAN.\n",
    "- Hough Transform: Another exploration to improve the SIFT method went through the usage of Hough transform, in particular, the idea was to pre-compute for each scene all possible book instances (of rectangular shape), and then run a simpler SIFT on the found rectangular shapes"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
